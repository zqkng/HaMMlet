{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import hammlet\n",
    "import hmm\n",
    "import sonnet_helper as sh\n",
    "import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer.py\n",
    "## (default sonnet text data: 'data/shakespeare.txt' and 'data/spenser.txt)\n",
    "## ========================================================================\n",
    "\n",
    "data = tokenizer.load_data()\n",
    "#print(data)\n",
    "\n",
    "lines = tokenizer.sequence_each_line(tokenizer.tokenize_lpunc, data)\n",
    "print(lines)\n",
    "\n",
    "quatrains, couplets = tokenizer.sequence_quatrains_couplets(tokenizer.tokenize_lpunc, data)\n",
    "#print(quatrains)\n",
    "#print(couplets)\n",
    "\n",
    "quatrain_rhymes, couplet_rhymes = tokenizer.process_rhymes(data)\n",
    "#print(quatrain_rhymes)\n",
    "#print(couplet_rhymes)\n",
    "\n",
    "word_count = tokenizer.process_word_frequency(data)\n",
    "#print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokenizer.py: rhymes processing\n",
    "## ===============================\n",
    "\n",
    "def generate_token_dict(training_sequences):\n",
    "    obs_id = 0\n",
    "    token_dict = {}\n",
    "\n",
    "    for sequence in training_sequences:\n",
    "        for token in sequence:\n",
    "            if token not in token_dict:\n",
    "                token_dict[token] = obs_id\n",
    "                obs_id += 1\n",
    "\n",
    "    return token_dict\n",
    "\n",
    "data = tokenizer.load_data()\n",
    "training_sequences = tokenizer.sequence_each_line(tokenizer.tokenize_lpunc, data)\n",
    "token_dict = generate_token_dict(training_sequences)\n",
    "\n",
    "q_rhymes, c_rhymes = tokenizer.process_rhymes(data)\n",
    "\n",
    "for r in q_rhymes:\n",
    "    try:\n",
    "        index0 = token_dict[r[0]]\n",
    "        index1 = token_dict[r[1]]\n",
    "    except KeyError:\n",
    "        print(\"Mising Key: {}\".format(r))\n",
    "        \n",
    "for c in c_rhymes:\n",
    "    try:\n",
    "        index0 = token_dict[c[0]]\n",
    "        index1 = token_dict[c[1]]\n",
    "    except KeyError:\n",
    "        print(\"Missing Key: {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hmm.py: train/save/load model\n",
    "## =============================\n",
    "\n",
    "# Specify number of states\n",
    "hmm_model = hmm.HiddenMarkovModel(10)\n",
    "# Specify stopping conditions\n",
    "A, B, PI, token_dict = hmm_model.train(lines, epsilon=0.1, max_iter=20)\n",
    "\n",
    "hmm.save_hmm_model(\"HMM-10S-01E-20X\", A, B, PI, token_dict)\n",
    "A2, B2, PI2, token_dict2 = hmm.load_hmm_model(\"HMM-10S-01E-20X\")\n",
    "\n",
    "assert(A.all() == A2.all()), \"ERROR: `A` does not match.\"\n",
    "assert(B.all() == B2.all()), \"ERROR: `B` does not match.\"\n",
    "assert(PI.all() == PI2.all()), \"ERROR: `PI` does not match.\"\n",
    "assert(token_dict == token_dict2), \"ERROR: `token_dict` does not match.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hammlet.py: generate sonnets\n",
    "## ============================\n",
    "\n",
    "#sonnet = hammlet.generate_sonnet(model_type=\"HMM\", model_name=\"HMM-10S-01E-20X\", rhyme=True)\n",
    "sonnet = hammlet.generate_hmm_sonnet(model=\"HMM-10S-01E-20X\", rhyme=True)\n",
    "print(sonnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
